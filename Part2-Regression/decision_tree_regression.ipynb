{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Position_Salaries.csv')\n",
    "X = df.iloc[:, 1:-1].values\n",
    "y = df.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training decision tree regression on whole dataset\n",
    "No feature scaling is necessary since the decision trees split the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(random_state=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "regressor = DecisionTreeRegressor(random_state=0)\n",
    "regressor.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on DecisionTreeRegressor in module sklearn.tree._classes object:\n",
      "\n",
      "class DecisionTreeRegressor(sklearn.base.RegressorMixin, BaseDecisionTree)\n",
      " |  DecisionTreeRegressor(*, criterion='mse', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, presort='deprecated', ccp_alpha=0.0)\n",
      " |  \n",
      " |  A decision tree regressor.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <tree>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  criterion : {\"mse\", \"friedman_mse\", \"mae\"}, default=\"mse\"\n",
      " |      The function to measure the quality of a split. Supported criteria\n",
      " |      are \"mse\" for the mean squared error, which is equal to variance\n",
      " |      reduction as feature selection criterion and minimizes the L2 loss\n",
      " |      using the mean of each terminal node, \"friedman_mse\", which uses mean\n",
      " |      squared error with Friedman's improvement score for potential splits,\n",
      " |      and \"mae\" for the mean absolute error, which minimizes the L1 loss\n",
      " |      using the median of each terminal node.\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |         Mean Absolute Error (MAE) criterion.\n",
      " |  \n",
      " |  splitter : {\"best\", \"random\"}, default=\"best\"\n",
      " |      The strategy used to choose the split at each node. Supported\n",
      " |      strategies are \"best\" to choose the best split and \"random\" to choose\n",
      " |      the best random split.\n",
      " |  \n",
      " |  max_depth : int, default=None\n",
      " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      " |      all leaves are pure or until all leaves contain less than\n",
      " |      min_samples_split samples.\n",
      " |  \n",
      " |  min_samples_split : int or float, default=2\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_split` as the minimum number.\n",
      " |      - If float, then `min_samples_split` is a fraction and\n",
      " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      " |        number of samples for each split.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_samples_leaf : int or float, default=1\n",
      " |      The minimum number of samples required to be at a leaf node.\n",
      " |      A split point at any depth will only be considered if it leaves at\n",
      " |      least ``min_samples_leaf`` training samples in each of the left and\n",
      " |      right branches.  This may have the effect of smoothing the model,\n",
      " |      especially in regression.\n",
      " |  \n",
      " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      " |      - If float, then `min_samples_leaf` is a fraction and\n",
      " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      " |        number of samples for each node.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, default=0.0\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |  \n",
      " |  max_features : int, float or {\"auto\", \"sqrt\", \"log2\"}, default=None\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |      - If int, then consider `max_features` features at each split.\n",
      " |      - If float, then `max_features` is a fraction and\n",
      " |        `int(max_features * n_features)` features are considered at each\n",
      " |        split.\n",
      " |      - If \"auto\", then `max_features=n_features`.\n",
      " |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      " |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      " |      - If None, then `max_features=n_features`.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  \n",
      " |  random_state : int, RandomState instance, default=None\n",
      " |      Controls the randomness of the estimator. The features are always\n",
      " |      randomly permuted at each split, even if ``splitter`` is set to\n",
      " |      ``\"best\"``. When ``max_features < n_features``, the algorithm will\n",
      " |      select ``max_features`` at random at each split before finding the best\n",
      " |      split among them. But the best found split may vary across different\n",
      " |      runs, even if ``max_features=n_features``. That is the case, if the\n",
      " |      improvement of the criterion is identical for several splits and one\n",
      " |      split has to be selected at random. To obtain a deterministic behaviour\n",
      " |      during fitting, ``random_state`` has to be fixed to an integer.\n",
      " |      See :term:`Glossary <random_state>` for details.\n",
      " |  \n",
      " |  max_leaf_nodes : int, default=None\n",
      " |      Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |  \n",
      " |  min_impurity_decrease : float, default=0.0\n",
      " |      A node will be split if this split induces a decrease of the impurity\n",
      " |      greater than or equal to this value.\n",
      " |  \n",
      " |      The weighted impurity decrease equation is the following::\n",
      " |  \n",
      " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      " |                              - N_t_L / N_t * left_impurity)\n",
      " |  \n",
      " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      " |  \n",
      " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      " |      if ``sample_weight`` is passed.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  min_impurity_split : float, (default=0)\n",
      " |      Threshold for early stopping in tree growth. A node will split\n",
      " |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      " |  \n",
      " |      .. deprecated:: 0.19\n",
      " |         ``min_impurity_split`` has been deprecated in favor of\n",
      " |         ``min_impurity_decrease`` in 0.19. The default value of\n",
      " |         ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n",
      " |         will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n",
      " |  \n",
      " |  presort : deprecated, default='deprecated'\n",
      " |      This parameter is deprecated and will be removed in v0.24.\n",
      " |  \n",
      " |      .. deprecated:: 0.22\n",
      " |  \n",
      " |  ccp_alpha : non-negative float, default=0.0\n",
      " |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      " |      subtree with the largest cost complexity that is smaller than\n",
      " |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      " |      :ref:`minimal_cost_complexity_pruning` for details.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  feature_importances_ : ndarray of shape (n_features,)\n",
      " |      The feature importances.\n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the\n",
      " |      (normalized) total reduction of the criterion brought\n",
      " |      by that feature. It is also known as the Gini importance [4]_.\n",
      " |  \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |  \n",
      " |  max_features_ : int\n",
      " |      The inferred value of max_features.\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      The number of features when ``fit`` is performed.\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      The number of outputs when ``fit`` is performed.\n",
      " |  \n",
      " |  tree_ : Tree\n",
      " |      The underlying Tree object. Please refer to\n",
      " |      ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\n",
      " |      :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\n",
      " |      for basic usage of these attributes.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  DecisionTreeClassifier : A decision tree classifier.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The default values for the parameters controlling the size of the trees\n",
      " |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      " |  unpruned trees which can potentially be very large on some data sets. To\n",
      " |  reduce memory consumption, the complexity and size of the trees should be\n",
      " |  controlled by setting those parameter values.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  \n",
      " |  .. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\n",
      " |  \n",
      " |  .. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification\n",
      " |         and Regression Trees\", Wadsworth, Belmont, CA, 1984.\n",
      " |  \n",
      " |  .. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical\n",
      " |         Learning\", Springer, 2009.\n",
      " |  \n",
      " |  .. [4] L. Breiman, and A. Cutler, \"Random Forests\",\n",
      " |         https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.datasets import load_diabetes\n",
      " |  >>> from sklearn.model_selection import cross_val_score\n",
      " |  >>> from sklearn.tree import DecisionTreeRegressor\n",
      " |  >>> X, y = load_diabetes(return_X_y=True)\n",
      " |  >>> regressor = DecisionTreeRegressor(random_state=0)\n",
      " |  >>> cross_val_score(regressor, X, y, cv=10)\n",
      " |  ...                    # doctest: +SKIP\n",
      " |  ...\n",
      " |  array([-0.39..., -0.46...,  0.02...,  0.06..., -0.50...,\n",
      " |         0.16...,  0.11..., -0.73..., -0.30..., -0.00...])\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      DecisionTreeRegressor\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      BaseDecisionTree\n",
      " |      sklearn.base.MultiOutputMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, criterion='mse', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, presort='deprecated', ccp_alpha=0.0)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None, check_input=True, X_idx_sorted=None)\n",
      " |      Build a decision tree regressor from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The target values (real numbers). Use ``dtype=np.float64`` and\n",
      " |          ``order='C'`` for maximum efficiency.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node.\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      X_idx_sorted : array-like of shape (n_samples, n_features),             default=None\n",
      " |          The indexes of the sorted training input samples. If many tree\n",
      " |          are grown on the same dataset, this allows the ordering to be\n",
      " |          cached between trees. If None, the data will be sorted here.\n",
      " |          Don't use this parameter unless you know what to do.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : DecisionTreeRegressor\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  classes_\n",
      " |  \n",
      " |  n_classes_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the coefficient of determination R^2 of the prediction.\n",
      " |      \n",
      " |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      " |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      " |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      " |      The best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always\n",
      " |      predicts the expected value of y, disregarding the input features,\n",
      " |      would get a R^2 score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples. For some estimators this may be a\n",
      " |          precomputed kernel matrix or a list of generic objects instead,\n",
      " |          shape = (n_samples, n_samples_fitted),\n",
      " |          where n_samples_fitted is the number of\n",
      " |          samples used in the fitting for the estimator.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True values for X.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          R^2 of self.predict(X) wrt. y.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The R2 score used when calling ``score`` on a regressor uses\n",
      " |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      " |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      " |      This influences the ``score`` method of all the multioutput\n",
      " |      regressors (except for\n",
      " |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseDecisionTree:\n",
      " |  \n",
      " |  apply(self, X, check_input=True)\n",
      " |      Return the index of the leaf that each sample is predicted as.\n",
      " |      \n",
      " |      .. versionadded:: 0.17\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array-like of shape (n_samples,)\n",
      " |          For each datapoint x in X, return the index of the leaf x\n",
      " |          ends up in. Leaves are numbered within\n",
      " |          ``[0; self.tree_.node_count)``, possibly with gaps in the\n",
      " |          numbering.\n",
      " |  \n",
      " |  cost_complexity_pruning_path(self, X, y, sample_weight=None)\n",
      " |      Compute the pruning path during Minimal Cost-Complexity Pruning.\n",
      " |      \n",
      " |      See :ref:`minimal_cost_complexity_pruning` for details on the pruning\n",
      " |      process.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The target values (class labels) as integers or strings.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. Splits are also\n",
      " |          ignored if they would result in any single class carrying a\n",
      " |          negative weight in either child node.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      ccp_path : :class:`~sklearn.utils.Bunch`\n",
      " |          Dictionary-like object, with the following attributes.\n",
      " |      \n",
      " |          ccp_alphas : ndarray\n",
      " |              Effective alphas of subtree during pruning.\n",
      " |      \n",
      " |          impurities : ndarray\n",
      " |              Sum of the impurities of the subtree leaves for the\n",
      " |              corresponding alpha value in ``ccp_alphas``.\n",
      " |  \n",
      " |  decision_path(self, X, check_input=True)\n",
      " |      Return the decision path in the tree.\n",
      " |      \n",
      " |      .. versionadded:: 0.18\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      indicator : sparse matrix of shape (n_samples, n_nodes)\n",
      " |          Return a node indicator CSR matrix where non zero elements\n",
      " |          indicates that the samples goes through the nodes.\n",
      " |  \n",
      " |  get_depth(self)\n",
      " |      Return the depth of the decision tree.\n",
      " |      \n",
      " |      The depth of a tree is the maximum distance between the root\n",
      " |      and any leaf.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self.tree_.max_depth : int\n",
      " |          The maximum depth of the tree.\n",
      " |  \n",
      " |  get_n_leaves(self)\n",
      " |      Return the number of leaves of the decision tree.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self.tree_.n_leaves : int\n",
      " |          Number of leaves.\n",
      " |  \n",
      " |  predict(self, X, check_input=True)\n",
      " |      Predict class or regression value for X.\n",
      " |      \n",
      " |      For a classification model, the predicted class for each sample in X is\n",
      " |      returned. For a regression model, the predicted value based on X is\n",
      " |      returned.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The predicted classes, or the predict values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from BaseDecisionTree:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      Return the feature importances.\n",
      " |      \n",
      " |      The importance of a feature is computed as the (normalized) total\n",
      " |      reduction of the criterion brought by that feature.\n",
      " |      It is also known as the Gini importance.\n",
      " |      \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : ndarray of shape (n_features,)\n",
      " |          Normalized total reduction of criteria by feature\n",
      " |          (Gini importance).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Overview of documentation:\n",
    "help(DecisionTreeRegressor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting a new result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([150000.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.predict([[6.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the decision tree (high resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_grid = np.arange(min(X), max(X), 0.1)\n",
    "X_grid = X_grid.reshape(len(X_grid), 1)\n",
    "plt.scatter(X, y, color='red')\n",
    "plt.plot(X_grid, regressor.predict(X_grid), color='blue')\n",
    "plt.title('Decision tree regression')\n",
    "plt.xlabel('Level of experience')\n",
    "plt.ylabel('Salary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
